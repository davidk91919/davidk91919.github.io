---
layout: post
title: 'note'
---

- On Nie and Wager(2021)

논문을 제대로 훑어봄. 참 부끄러운 얘긴데 이제야 이 논문의 핵심을 좀 알 것 같다. 기존에는 그냥 semi-parameteric setting으로의 확장이 메인 advantage라고 생각을 했는데
지금 보니 크게 세 가지의 advantage에 대해서 말하고 있다: 

(1) semi-parameteric setting으로의 확장 

(2) k-fold estimate의 과정에서, 각 단계마다 다른 러닝 방법들을 'stack'할 수 있음; m이나 g에 대한 estimate와 헷갈리지 말아야한다. 얘네들은 이미 각 단계에 '주어져있다고'(oracle) 가정이 된거고 그 상황에서 tau를 estimate할 때, 각 fold에 대해 다른 estimate들을 최종적으로 combine할 수 있다는 것(캬 아이디어 개멋지네) 

(3) 그리고 추가적으로 kernel methods에 대해서(그러니까 RKHS같은 것들에 대해서)의 특정 조건 아래에서 (예를 들어 almost surely bounded된 outcomes) 좋은 performances.

오.. 생각하던 것보다 훠얼씬 멋진 논문이었다.
