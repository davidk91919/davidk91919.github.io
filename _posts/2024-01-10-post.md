---
layout: post
title: 'note'
---

- 일본
짧게 일본 다녀왔다. 부모님 모시고 패키지로 다녀왔다. 아무 생각(혹은 기대)없이 다녀왔는데 매우 인상깊고 즐겁게 다녀옴.. 얼마전 교토 다녀온 동생이 교토를 찬양하길래 그런가보다 했는데 나도 어떤 느낌인지 이해를 함. 또 이번에 한국 나온 준하도 그렇게 일본 찬양을 했는데 그것도 이해가 갈 정도. 나중에 괜찮은 잡이 있으면 일본으로 취직하고 싶다는 생각이 생겼다.. 봄에 비행기표 싼 거 나오면 또 가고 싶음. 준하랑 가든 다른 친구랑 가든 꼭 가고 싶다. 

- On Tsiatis's Semiparameteric Theory
중간중간 아이패드로 책 논문 찔끔씩 읽다옴. Tsiatis 책은 4장을 읽고 있는데 염두에 두고 있는 게 semiparameteric 셋팅이라는 걸 생각하면 완전 직접적으로 관련 있는 챕터를 읽고 있는 셈.

통계를 한다는 것의 spirit이라는 걸 느낄 수 있었는데 parameteric submodel이라는 걸 보면서 그랬다. 무한 차원에 있는 모르는 대상을 다루기 위해 아는 것들로부터 모델을 구성하고 '우리가 모른다면 얼마나 모르는지' 확인하려는 발상. 확실하지 않은 지식들에 대해 무의미하다고 간주하는 학문들의 철학과 대비되는 통계만의 철학이라고 할 수 있을 듯.

또 방금은 3장으로 돌아가서 estimator에서 unbiasedness가 깨지면 geometry가 어떻게 튀는지 더 자세히 확인함. 구체적으론 (finite dimension에서의 m-estimator 셋팅에서) regularity를 이용해서 결국 beta의 information matrix를 identity matrix로 characterize하는 게 큰 아이디어라고 볼 수 있는데, unbiasedness가 깨지면 그게 identity로 가지 않게 된다는.

- Kernel Method

PRML의 Kernel Method에 대한 챕터도 읽기 시작했는데 Dual Representation이라는 걸 하는 모티베이션을 잘 모르겠어서 고민을 했다. 처음에는 대수에서의 Dual Representation이랑 관련이 있나해서 뒤져봤는데, 만약 둘이 같은 이야기더라도 능력적으로든 시간적으로든 내가 아직 건드릴 곳이 아닌 거 같아 깨갱하고 돌아섬. 대신 모티베이션을 얻긴 했는데 - x들로부터 feature를 일일이 구체적으로 구성하는 것 대신에, feature들을 일종의 inner space의 element로 간주한 다음 그들에 대한 대략적인 스케치 - 즉 covariance에 대한 정보를 통해 분석을 하겠다는 것. 즉 K(x, x') = Phi(x) Phi(x')라고 할 때 뒤에 부분은 결국 feature들의 covariance에 대한 정보를 담고 있고 non-linear stuff들에 대해선 직접 x에 대해 일할 수 없으니 대신 얘네들에 대해서 작업하겠다는 것. 재밌당 재밌어 근데 논문까지 쓰기엔 아직 좀 어렵네..

