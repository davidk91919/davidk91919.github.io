---
layout: post
title: 'note'
---

- 한 것

내일 미팅 있어서 준비했다. 시뮬레이션 돌리고 플롯 만들고 증명 하나 하고 기타 등등.. 그러다가 교수님의 다른 document 하나를 봤다. 다음 시뮬레이션 방향을 어떻게 잡아야하나 막막하던 참이었는데 이걸 읽어보면 될듯. 근데 아직 읽진 않음. 내일 조교 수업 준비하고 운동 다녀왔다. 어깨가 매우 잘 먹었다 = 비타민 안챙겨먹으면 내일 컨디션 바닥일 것. 꼭 챙겨먹기. 어깨 하면서 수프얀의 캐리로웰 앨범을 들었다. 오랜만에 들으니 더 좋고 운동할 때 들으니 또 너무 좋았다 참 좋은 앨범.. 그리고 돌아와서 조교일 좀 더 하고.

- 할 것

과제를 좀 하다갈 것 같다. 그리고 교수님 문서 읽어보기. 코딩 더블 체크도 하려고 했는데 하루종일 코딩만 하니까 그러기가 싫다 내일 해야지.. 

- Some Thoughts

운동하다가 딥러닝 모델의 퍼포먼스가 왜 좋을까 생각을 해봤다. 그러니까 파라미터 갯수가 엄청나게 많을 때 왜 gradient descent로 구한 local minima가 왜 global maxima에 근접하는지. 최소한 왜 그런 것처럼 행동하는지. parameter 갯수와 그것들로부터 span된(?) 공간이 convex해져서 그런가? 는 생각이 들었고 그러면서 그런 공간의 뭐랄까 geometry가 일반적으로 궁금해졌다. 그러니까 orthogonality로 convexity가 해명될 수 있는 부분이 있는지. 또 그걸 공간의 관점에서 바라볼 때 달라지는 것 / 예쁜 결과들이 있을지. 그런 의미에서 Conway책을 좀 읽고 있고.. 여튼 가볍게 서치해봐야겠다. 

