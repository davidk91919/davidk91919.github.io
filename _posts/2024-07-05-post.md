---
layout: post
title: 'research notes(1) - about KAN'
---

일기와 리서치 노트들을 조금 구분해두는 게 낫겠다고 생각했다. 카테고리는 만들기 귀찮..

- KAN

오랜만에 머신러닝하는 분들과 스터디를 했다. 약 한달만.. 내가 발제하는 차례였고 최근 핫한 KAN에 대해서 발제함. 굉장히 재밌게 읽었고 또 발제를 했는데, KAN이라는 프레임워크를 살펴본 감상들을 이야기해보자면:

1. 두 가지 핵심 직관에서 출발한 프레임워크라는 생각을 했다. (1) polynomial approximation을 차원의 저주없이 할 수는 없나? (2) 기존에 MLP(혹은 UAN에 기반을 둔 일반적인 프레임워크들)보다 더 interpretable한 프레임워크는 없을까?

2. 그런 의미에서 KAN이 KAT 즉 Kolmogorov-Arnold Theorem에서 얻은 아이디어는 각 layer의 input에 대한 vector-wise linear weight들을 고려하지 말고, 그 벡터를 element-wise 쪼개서 각각의 1 dimension function들을 쪼개서 튜닝해보자는 거다. 그리고 튜닝의 대상이 1 dimension이 되었기 때문에 차원의 저주에 시달렸던 spline들로 각 대상들을 parametrize할 수 있게 됨.

3. 다음 얘기를 하기 전에 잠깐 첨언하자면, 이름과는 달리 KAN과 KAT의 관계는 MLP와 UAN의 그것보다 훨씬 약하다. 즉 후자에 있어서 UAN이 MLP의 approximation bound에 대한 수학적인 결과를 직접적으로 제공해준다면, 그러나 전자는 애초에 representation theorem이고 그러므로 Kolmogorov와 Arnold가 했던 수학적인 작업에서 KAN은 영감을 받은 것이지, 그 정리에서 네트워크의 형태와 수학적인 정당화가 직접 주어지는 것은 아니다.

4. 고럼 여기서 드는 자연스러운 의문들은.. (1) 그럼 KAN의 approximation bound는 어디서 옴? (2) element-wisely spline function들을 tuning하면 efficiency 측면에서 MLP보다 훨씬 뒤쳐지는 거 아님?

5. (1). 그런 점에서 논문의 (수학적) 핵심은 정리 2.1이라고 할 수 있겠다. 이 정리는, 일종의 목표가 되는 함수가 kth continuously differentiable한 KA representation을 허용한다면, spline으로 구성된 KAN이 그 representation에서 따라오는 상수와 그리고 grid size에 의해 characterize되는 bound를 갖게 된다- 는 무척 놀라운 결과를 함축한다. 왜 놀랍냐면 bound에서 input dimension으로 형상화되는 차원과 관련된 term이 없기 때문이고 간단하게 말하자면, 정리 자체에서는 차원의 저주가 발견되지 않기 때문이다.

6. 그러나 (1) 그런 representation을 허용하는 함수의 family랄까 그런 게 있냐? (2) 그런 게 없으면 C가 결국은 차원과 관련된 상수가 되는 게 아니냐? 라는 게 자연스러운 의문이 될 것이고.. 저자들은 이 점에 있어서 어떤 명시적인 family를 제공한다기보다 여러 다양한 예시들을 가지고 실험을 하는 것 같다. 즉, 그런 명시적인 family와 그 family의 범위가 주어지지 않는다면 이 정리 자체는 'KAN을 쓰면 차원의 저주가 발생하지 않음!'을 바로 함축하지는 못한다는 것.

7. (2) 그래서 저자들이 pruning 과정과 sparsification 과정에 집중을 한다는 생각을 했다. 이 부분이 굉장히 재밌는데.. 즉 아예 유저 인터페이스로 이 과정을 포함시켜서 interpretability를 높이자! 이런 의도가 엿보여서 그런 것. 무슨 얘기냐면, 최초의 sparsification을 거친 모델이 주어졌을 때 일단 그 모델에서 영향력이 낮은 node들을 visualization을 한다. 그리고 그런 visualization을 봤을 때 매우 흐리게 나오는 node들, 혹은 threshold(1/100?)을 통과하지 못한 node들을 지운다. 그리고 남은 node들에서는 그 node들이 표상하는 함수과 닮은 잘 알려진 함수가 있다면 그냥 그 함수를 복사 붙여넣기 한다.

8. 그렇다면 결과함수 자체도 더 -삼각함수나 초월함수 등으로 표현될 것이기 때문에- interpretable 해질 것이고.. 또 각 과정에서 유저가 개입하는 측면이 커졌기 때문에, '문제를 설정하고, 모델들을 선택하고 최종 결과를 선택한다'는 넓은 의미에서의 efficiency도 증가한다고 생각할 수 있을 것 같다(이건 저자들이 말한 건 아니고 내가 생각한거임).

9. 정리를 해보자면.. 1. KAT에서 영향을 받은 1-dimensional functions tuning이라는 프레임워크를 적용했기 때문에 차원의 저주를 이길 수도 있는 spline based tuning이 가능해진다. 2. 또 spline을 썼기 때문에도 그렇고 그리고 그 spline에서 이어지는 norm들을 규정했기 때문에도 그렇고 여러가지 점에서 model interpretability가 더 좋다. 3. efficiency는 명시적으로 이야기하고는 않지만 여러가지 example들을 통해 보여주려고 하고 있는 것 같다.

10. 함축과 limitation을 생각해보자면.. (1) 확실히 더 복잡한 형태의 목적함수들(초월함수들로 구성되어서 linear approximation이 좀 힘들다거나 하는)에 대해서는 KAN이 퍼포먼스가 더 좋을 것 같다. 다시 말해서, 공학이나 이론과학에서 직접적으로 머신러닝을 쓰겠다고 했을 때는 KAN이 MLP보다 매력적인 초이스가 될 수도 있을 듯 함. (2) 근데 efficiency 부분은 뭐랄까 아직 와닿지는 않는다. 저자들이 대충 말하고 지나간 느낌? 어쨌든 MLP와 비교했을 때 spline grid size로 characterize한만큼의 efficiency loss가 있을텐데.. 흠 아직까지는 이 부분이 잘 납득은 안됨. 

- DML

에 대한 얘기도 좀 하고 싶었던 거 같다. 근데 발제까지 끝내니까 넘 피곤하다. 언젠간 적겠지(?)

-

+ 스터디원이 공유해주신 재밌어보이는 사이트. 아마 KAN을 웹에서 구현하게 만든 거 같음 https://kanvas.deepverse.tech/#/kan





